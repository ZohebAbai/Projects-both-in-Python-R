{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: NLP\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the texts\n",
    "#install.packages('tm')\n",
    "#install.packages('SnowballC')\n",
    "library(tm)\n",
    "library(SnowballC)\n",
    "corpus = VCorpus(VectorSource(dataset_original$Review))\n",
    "corpus = tm_map(corpus, content_transformer(tolower))\n",
    "corpus = tm_map(corpus, removeNumbers)\n",
    "corpus = tm_map(corpus, removePunctuation)\n",
    "corpus = tm_map(corpus, removeWords, stopwords())\n",
    "corpus = tm_map(corpus, stemDocument)\n",
    "corpus = tm_map(corpus, stripWhitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as.character(corpus[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "dtm = DocumentTermMatrix(corpus)\n",
    "dtm = removeSparseTerms(dtm, 0.999)\n",
    "dataset = as.data.frame(as.matrix(dtm))\n",
    "dataset$Liked = dataset_original$Liked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the target feature as factor\n",
    "dataset$Liked = factor(dataset$Liked, levels = c(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "# install.packages('caTools')\n",
    "library(caTools)\n",
    "set.seed(123)\n",
    "split = sample.split(dataset$Liked, SplitRatio = 0.8)\n",
    "training_set = subset(dataset, split == TRUE)\n",
    "test_set = subset(dataset, split == FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n"
     ]
    }
   ],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "# install.packages('randomForest')\n",
    "library(randomForest)\n",
    "classifier = randomForest(x = training_set[-692],\n",
    "                          y = training_set$Liked,\n",
    "                          ntree = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = predict(classifier, newdata = test_set[-692])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   y_pred\n",
       "     0  1\n",
       "  0 82 18\n",
       "  1 23 77"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "table(test_set[, 692], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
